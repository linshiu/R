---
title: "MSIA 401 - Homework #7"
output: word_document
---
# MSIA 401 - Hw7
## *Steven Lin*

# Setup

```r
# My PC
main = "C:/Users/Steven/Documents/Academics/3_Graduate School/2014-2015 ~ NU/"

# Aginity main = '\\\\nas1/labuser169'

course = "MSIA_401_Statistical Methods for Data Mining"
datafolder = "Data"
setwd(file.path(main, course, datafolder))

opts_knit$set(root.dir = getwd())
```


# Problem 1


```r

# Import data
filename = "P012.txt"
mydata = read.table(filename, header = T)

# Look at data
names(mydata)
head(mydata)
nrow(mydata)
summary(mydata)
```


## Part a

**It makes more sense to find the probability of at least one O-ring failure since an O-ring prevents the rocket from exploding, but problem is not asking to model probability of at least one failure, but rather the probability of failure (similar to Problem 2, where you woulndn't create a binary variable to model the probability of success as at least one success in the attempts).** 


```r

# create binary variable
mydata = data.frame(Damaged_bin = ifelse(mydata$Damaged >= 1, 1, 0), mydata)

# fit logistic
fit = glm(cbind(Damaged, 6 - Damaged) ~ Temp, family = binomial(link = "logit"), 
    data = mydata)
fit2 = glm(Damaged_bin ~ Temp, family = binomial(link = "logit"), data = mydata)

# Plot
# http://www.shizukalab.com/toolkits/plotting-logistic-regression-in-r
# http://www.harding.edu/fmccown/r/ Legend
# http://www.statmethods.net/advgraphs/axes.html

temps = seq(30, 80, 1)

plot(mydata$Temp, mydata$Damaged_bin, pch = 16, xlab = "Temperature (F)", ylab = "Actual outcome and Predicted Probabilities", 
    xlim = c(15, 85))

curve(predict(fit, data.frame(Temp = x), type = "resp"), add = TRUE, col = "Blue", 
    lwd = 2)
points(mydata$Temp, predict(fit, data.frame(Temp = mydata$Temp), type = "resp"), 
    pch = 16, col = "Blue")

curve(predict(fit2, data.frame(Temp = x), type = "resp"), add = TRUE, col = "Red", 
    lwd = 2)
points(mydata$Temp, predict(fit2, data.frame(Temp = mydata$Temp), type = "resp"), 
    pch = 16, col = "Red")

legend("bottomleft", inset = 0.05, c("Fit1", "Fit2"), col = c("Blue", "Red"), 
    lty = 1)
```

![plot of chunk unnamed-chunk-1](figure/unnamed-chunk-1.png) 

```r

summary(fit)
```

```
## 
## Call:
## glm(formula = cbind(Damaged, 6 - Damaged) ~ Temp, family = binomial(link = "logit"), 
##     data = mydata)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -0.9523  -0.7830  -0.5412  -0.0438   2.6515  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)  
## (Intercept)    5.085      3.053    1.67    0.096 .
## Temp          -0.116      0.047   -2.46    0.014 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 24.230  on 22  degrees of freedom
## Residual deviance: 18.086  on 21  degrees of freedom
## AIC: 35.65
## 
## Number of Fisher Scoring iterations: 5
```

```r
summary(fit2)
```

```
## 
## Call:
## glm(formula = Damaged_bin ~ Temp, family = binomial(link = "logit"), 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.061  -0.761  -0.378   0.452   2.217  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)  
## (Intercept)   15.043      7.379    2.04    0.041 *
## Temp          -0.232      0.108   -2.14    0.032 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 28.267  on 22  degrees of freedom
## Residual deviance: 20.315  on 21  degrees of freedom
## AIC: 24.32
## 
## Number of Fisher Scoring iterations: 5
```

```r

fit$coeff["(Intercept)"]
```

```
## (Intercept) 
##       5.085
```

```r
exp(fit$coeff["(Intercept)"])
```

```
## (Intercept) 
##       161.6
```

```r

fit$coeff["Temp"]
```

```
##    Temp 
## -0.1156
```

```r
exp(fit$coeff["Temp"])
```

```
##   Temp 
## 0.8908
```

```r
1 - exp(fit$coeff["Temp"])
```

```
##   Temp 
## 0.1092
```

```r

1/exp(fit$coeff["Temp"])
```

```
##  Temp 
## 1.123
```

```r
1/exp(fit$coeff["Temp"]) - 1
```

```
##   Temp 
## 0.1225
```

Note: could use binary variable that indicates damage of a least one O-ring (Damage_bin = 1 when Damage >=1, 0 otherwise). However, we would be losing information regarding the number of O-rings that were damaged out of the total 6 O-rings, meaning we will not be differentiating a temperature that had more than one O-ring failure vs a temperature that had only one O-ring failure. Thus, the number of O-rings that were damaged and the number of O-rings that did not get damaged are used as inputs to the model. All answers in this section refer to fit1 (using number of successes and failures). Results from fit2 (using binary) are shown in the summary above. Interpretations for fit2 would be the same except that we would be referring to  the probability of at least one O-ring failure.

For this problem, we are modeling the probability of an O-ring failure (i.e. damaged)

The intercept is just the expected log odds of an O-ring failure (versus no failure) when the temperature is zero degrees Farenheight. So here the expected log odds is 5.085, or the odds is 161.5763 when the temperature is 0 F. The model indicates that intercept is also significant at 0.05 level. 

The coefficients of the predictor variables indicate the expected change in the log odds of the outcome for a one-unit increase in the predictor variable. So in this example, the coefficient of temperature indicates that for every one-unit increase in the temperature (a degree Fahrenheit), the expected change in the log odds of an O-ring failure (versus no failure) is -0.1156 (or decrease 0.1156). Equivalently, for every one-unit decrease in the temperature (a degree Fahrenheit), the log odds of an O-ring failure (versus no failure) increases by 0.1156. Because the coefficient is negative, the probability of an O-ring failure is higher at lower temperatures. The Wald statistic and p-value < 0.05 indicates the effect of temperature on o-ring damage is significant.

A more intuitive intepretation is that the exponential of the coefficient of the predictor variables is the multiplicative factor by which the odds of the outcome is expected to change given a one-unit increase in the  predictor variable. So in this example,  the exponential of the coefficient of temperature indicates that for every one-unit increase in the temperature (a degree Fahrenheit), the expected odds of an O-ring failure (versus no failure) changes by a factor of 0.8908 (or decrease of 10.917 %). In other words, the odds of an O-ring failure (versus no failure) is expected to decrease by 10.917 % for each one-unit increase in the temperature (a degree Fahrenheit). Equivalently, for each one-unit decrease in the temperature (a degree Fahrenheit), the expected odds are multiplied by 1.1225 (or increase by 12.2548 %)

Alternatively, the exponential of the coefficient of the predictor variables is the odds ratio (odds if corresponding variable is increamented by 1 over odds if variable not incremented). Thus, the odds ratio is 0.8908 for a one-unit increase in the temperature (a degree Fahrenheit), meaning the probability of an O-ring failure (versus no failure) equals 1 is 0.8908 as likely as the value of the temperature is increased one unit (one degree Fahrenhiet). Equivalently, the  probability of an O-ring failure (versus no failure) equals 1 is 1.1225 as likely as the value of the temperature is decreased one unit (one degree Fahrenhiet).

## Part b


```r

# remove obs 18
mydata = subset(mydata, as.numeric(rownames(mydata)) != 18)
mydata
```

```
##    Damaged_bin Damaged Temp
## 1            1       2   53
## 2            1       1   57
## 3            1       1   58
## 4            1       1   63
## 5            0       0   66
## 6            0       0   67
## 7            0       0   67
## 8            0       0   67
## 9            0       0   68
## 10           0       0   69
## 11           0       0   70
## 12           0       0   70
## 13           1       1   70
## 14           1       1   70
## 15           0       0   72
## 16           0       0   73
## 17           0       0   75
## 19           0       0   76
## 20           0       0   78
## 21           0       0   79
## 22           0       0   81
## 23           0       0   76
```

```r

# fit logistic
fit = glm(cbind(Damaged, 6 - Damaged) ~ Temp, family = binomial(link = "logit"), 
    data = mydata)
fit2 = glm(Damaged_bin ~ Temp, family = binomial(link = "logit"), data = mydata)

# Plot
# http://www.shizukalab.com/toolkits/plotting-logistic-regression-in-r
# http://www.harding.edu/fmccown/r/ Legend
# http://www.statmethods.net/advgraphs/axes.html

temps = seq(30, 80, 1)

plot(mydata$Temp, mydata$Damaged_bin, pch = 16, xlab = "Temperature (F)", ylab = "Actual outcome and Predicted Probabilities", 
    xlim = c(15, 85))

curve(predict(fit, data.frame(Temp = x), type = "resp"), add = TRUE, col = "Blue", 
    lwd = 2)
points(mydata$Temp, predict(fit, data.frame(Temp = mydata$Temp), type = "resp"), 
    pch = 16, col = "Blue")

curve(predict(fit2, data.frame(Temp = x), type = "resp"), add = TRUE, col = "Red", 
    lwd = 2)
points(mydata$Temp, predict(fit2, data.frame(Temp = mydata$Temp), type = "resp"), 
    pch = 16, col = "Red")

legend("bottomleft", inset = 0.05, c("Fit1", "Fit2"), col = c("Blue", "Red"), 
    lty = 1)
```

![plot of chunk unnamed-chunk-2](figure/unnamed-chunk-2.png) 

```r

summary(fit)
```

```
## 
## Call:
## glm(formula = cbind(Damaged, 6 - Damaged) ~ Temp, family = binomial(link = "logit"), 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.761  -0.574  -0.332  -0.186   1.520  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)   
## (Intercept)   8.6616     3.6344    2.38   0.0172 * 
## Temp         -0.1768     0.0587   -3.01   0.0026 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 20.0667  on 21  degrees of freedom
## Residual deviance:  9.4096  on 20  degrees of freedom
## AIC: 24.75
## 
## Number of Fisher Scoring iterations: 6
```

```r
summary(fit2)
```

```
## 
## Call:
## glm(formula = Damaged_bin ~ Temp, family = binomial(link = "logit"), 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.003  -0.608  -0.206   0.106   2.006  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)  
## (Intercept)   23.403     11.832    1.98    0.048 *
## Temp          -0.361      0.176   -2.06    0.040 *
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 25.782  on 21  degrees of freedom
## Residual deviance: 14.377  on 20  degrees of freedom
## AIC: 18.38
## 
## Number of Fisher Scoring iterations: 6
```

```r

fit$coeff["(Intercept)"]
```

```
## (Intercept) 
##       8.662
```

```r
exp(fit$coeff["(Intercept)"])
```

```
## (Intercept) 
##        5777
```

```r

fit$coeff["Temp"]
```

```
##    Temp 
## -0.1768
```

```r
exp(fit$coeff["Temp"])
```

```
##   Temp 
## 0.8379
```

```r
1 - exp(fit$coeff["Temp"])
```

```
##   Temp 
## 0.1621
```

```r

1/exp(fit$coeff["Temp"])
```

```
##  Temp 
## 1.193
```

```r
1/exp(fit$coeff["Temp"]) - 1
```

```
##   Temp 
## 0.1934
```


Note: could use binary variable that indicates damage of a least one O-ring (Damage_bin = 1 when Damage >=1, 0 otherwise). However, we would be losing information regarding the number of O-rings that were damaged out of the total 6 O-rings, meaning we will not be differentiating a temperature that had more than one O-ring failure vs a temperature that had only one O-ring failure. Thus, the number of O-rings that were damaged and the number of O-rings that did not get damaged are used as inputs to the model. All answers in this section refer to fit1 (using number of successes and failures). Results from fit2 (using binary) are shown in the summary above. Interpretations for fit2 would be the same except that we would be referring to  the probability of at least one O-ring failure.

For this problem, we are modeling the probability of an O-ring failure (i.e. damaged)

The intercept is just the expected log odds of an O-ring failure (versus no failure) when the temperature is zero degrees Farenheight. So here the expected log odds is 8.6616, or the odds is 5776.5778 when the temperature is 0 F. The model indicates that intercept is also significant at 0.05 level. 

The coefficients of the predictor variables indicate the expected change in the log odds of the outcome for a one-unit increase in the predictor variable. So in this example, the coefficient of temperature indicates that for every one-unit increase in the temperature (a degree Fahrenheit), the expected change in the log odds of an O-ring failure (versus no failure) is -0.1768 (or decrease 0.1768). Equivalently, for every one-unit decrease in the temperature (a degree Fahrenheit), the log odds of an O-ring failure (versus no failure) increases by 0.1768. Because the coefficient is negative, the probability of an O-ring failure is higher at lower temperatures. The Wald statistic and p-value < 0.05 indicates the effect of temperature on o-ring damage is significant.

A more intuitive intepretation is that the exponential of the coefficient of the predictor variables is the multiplicative factor by which the odds of the outcome is expected to change given a one-unit increase in the  predictor variable. So in this example,  the exponential of the coefficient of temperature indicates that for every one-unit increase in the temperature (a degree Fahrenheit), the expected odds of an O-ring failure (versus no failure) changes by a factor of 0.8379 (or decrease of 16.2057 %). In other words, the odds of an O-ring failure (versus no failure) is expected to decrease by 16.2057 % for each one-unit increase in the temperature (a degree Fahrenheit). Equivalently, for each one-unit decrease in the temperature (a degree Fahrenheit), the expected odds are multiplied by 1.1934 (or increase by 19.3398 %)

Alternatively, the exponential of the coefficient of the predictor variables is the odds ratio (odds if corresponding variable is increamented by 1 over odds if variable not incremented). Thus, the odds ratio is 0.8379 for a one-unit increase in the temperature (a degree Fahrenheit), meaning the probability of an O-ring failure (versus no failure) equals 1 is 0.8379 as likely as the value of the temperature is increased one unit (one degree Fahrenhiet). Equivalently, the  probability of an O-ring failure (versus no failure) equals 1 is 1.1934 as likely as the value of the temperature is decreased one unit (one degree Fahrenhiet).

## Part c

```r
prob = predict(fit, data.frame(Temp = 31), type = "resp")  # resp -> converts to probabilities
signif(prob, 6)
```

```
##      1 
## 0.9601
```

```r
sprintf("%.6f", prob)
```

```
## [1] "0.960098"
```

```r

prob2 = predict(fit2, data.frame(Temp = 31), type = "resp")  # resp -> converts to probabilities
signif(prob2, 6)
```

```
## 1 
## 1
```

```r
sprintf("%.6f", prob2)
```

```
## [1] "0.999995"
```


The probability of an O-ring failure when temperature is 31 degree F is 0.960098. Using the binary fit, the probabilty of at least one O-ring failure is 0.999995.

## Part d

It is NOT advisable to launch on that particular day because the probability of an O-ring failure is very high, meaning it is extremely likely that an O-ring will fail on that day given the temperature of 31 degree F. The probability that at least one of the O-ring (out of the 6) would fail is almost 1 (using the binary fit, or 1 ~ P(at least one out of six will fail) = 1 - P(None of the six wil fail), where P(None of the six will fail) = (1-P(O-ring failure))^6 and P(O-ring failure) = 0.960098). Note that the probability that all six O-rings will fail is also high at 78.3239 % (P(all six fail)= P(O-ring failure)^6 , where P(O-ring failure)=0.960098).

However, the 31 degree F is outside the range of the predicted range of the model. So caution should be taken when making decisions based on the model. 

# Problem 2


```r

# Import data
filename = "P357.txt"
mydata = read.table(filename, header = T)

# Look at data
names(mydata)
head(mydata)
nrow(mydata)
summary(mydata)
```


## Part a

```r
#
# http://stats.stackexchange.com/questions/26762/how-to-do-logistic-regression-in-r-when-outcome-is-fractional
# http://www.stat.ufl.edu/~presnell/Courses/sta4504-2000sp/R/R-CDA.pdf

mydataNFL = subset(mydata, League == "NFL")
mydataAFL = subset(mydata, League == "AFL")

# input as Success, Failures
fitNFL = glm(cbind(Success, Attempts - Success) ~ Distance + I(Distance^2), 
    family = binomial(link = "logit"), data = mydataNFL)
fitAFL = glm(cbind(Success, Attempts - Success) ~ Distance + I(Distance^2), 
    family = binomial(link = "logit"), data = mydataAFL)

# Other option: input as Success/Total fitNFL = glm(Success/Attempts ~
# Distance + I(Distance^2), weights= Attempts,
# family=binomial(link='logit'), data=mydataNFL)

summary(fitNFL)
```

```
## 
## Call:
## glm(formula = cbind(Success, Attempts - Success) ~ Distance + 
##     I(Distance^2), family = binomial(link = "logit"), data = mydataNFL)
## 
## Deviance Residuals: 
##       1        2        3        4        5  
##  0.1163  -0.0005  -0.4017   0.6421  -0.9146  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)  
## (Intercept)    2.49020    1.01862    2.44    0.014 *
## Distance      -0.01317    0.06599   -0.20    0.842  
## I(Distance^2) -0.00151    0.00101   -1.50    0.134  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 147.7816  on 4  degrees of freedom
## Residual deviance:   1.4238  on 2  degrees of freedom
## AIC: 28.89
## 
## Number of Fisher Scoring iterations: 4
```

```r
summary(fitAFL)
```

```
## 
## Call:
## glm(formula = cbind(Success, Attempts - Success) ~ Distance + 
##     I(Distance^2), family = binomial(link = "logit"), data = mydataAFL)
## 
## Deviance Residuals: 
##      6       7       8       9      10  
##  0.319  -0.683   0.772  -0.523   0.285  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept)     4.8925     1.1893    4.11  3.9e-05 ***
## Distance       -0.1971     0.0743   -2.65    0.008 ** 
## I(Distance^2)   0.0016     0.0011    1.46    0.144    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 78.7794  on 4  degrees of freedom
## Residual deviance:  1.5192  on 2  degrees of freedom
## AIC: 28.44
## 
## Number of Fisher Scoring iterations: 3
```

## Part b

```r

fit = glm(cbind(Success, Attempts - Success) ~ Distance + I(Distance^2) + Z, 
    family = binomial(link = "logit"), data = mydata)
summary(fit)
```

```
## 
## Call:
## glm(formula = cbind(Success, Attempts - Success) ~ Distance + 
##     I(Distance^2) + Z, family = binomial(link = "logit"), data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.863  -0.201   0.033   0.555   1.601  
## 
## Coefficients:
##                Estimate Std. Error z value Pr(>|z|)    
## (Intercept)    3.524184   0.774783    4.55  5.4e-06 ***
## Distance      -0.095871   0.049021   -1.96     0.05 .  
## I(Distance^2) -0.000109   0.000737   -0.15     0.88    
## Z              0.103753   0.169831    0.61     0.54    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 228.5180  on 9  degrees of freedom
## Residual deviance:   8.9776  on 6  degrees of freedom
## AIC: 59.37
## 
## Number of Fisher Scoring iterations: 4
```


## Part c


```r
pvalue = summary(fit)$coef["I(Distance^2)", "Pr(>|z|)"]
pvalue
```

```
## [1] 0.8828
```


The p-value for the quadratic term is 0.8828 > 0.05, which indicates that the quadratic term is insignificant (cannot reject Ho that the coefficient is equal to zero given other variables in the model) and thus does NOT contribute significantly to the model.


## Part d


```r
pvalue = summary(fit)$coef["Z", "Pr(>|z|)"]
pvalue
```

```
## [1] 0.5413
```

```r

fit2 = update(fit, . ~ . - I(Distance^2))
summary(fit2)
```

```
## 
## Call:
## glm(formula = cbind(Success, Attempts - Success) ~ Distance + 
##     Z, family = binomial(link = "logit"), data = mydata)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -1.9179  -0.2547   0.0509   0.5778   1.5465  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(>|z|)    
## (Intercept)   3.6296     0.3031   11.97   <2e-16 ***
## Distance     -0.1030     0.0081  -12.71   <2e-16 ***
## Z             0.1036     0.1698    0.61     0.54    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 228.5180  on 9  degrees of freedom
## Residual deviance:   8.9993  on 7  degrees of freedom
## AIC: 57.39
## 
## Number of Fisher Scoring iterations: 4
```

```r
pvalue2 = summary(fit)$coef["Z", "Pr(>|z|)"]
pvalue2
```

```
## [1] 0.5413
```

```r

# odds ratios and 95% CI
exp(cbind(OR = coef(fit), confint(fit)))
```

```
## Waiting for profiling to be done...
```

```
##                    OR  2.5 %   97.5 %
## (Intercept)   33.9261 7.7771 163.5667
## Distance       0.9086 0.8239   0.9989
## I(Distance^2)  0.9999 0.9985   1.0013
## Z              1.1093 0.7951   1.5481
```


The p-value for the Z term is 0.5413 > 0.05, which indicates that the Z term (league indicator) is insignificant (cannot reject Ho that the coefficient is equal to zero given other variables in the model) and thus does NOT contribute significantly to the model. In other words, because the effect of the leauge is insignificant after taking into account distance and distance^2, then the probabilities of scoring field goals from a given distance and distance^2 are NOT statistically different for each league (i.e. probabilities are the same for each league). 

Removing the insignificant quadratic term, the p-value for the Z term is 0.5413 > 0.05, which indicates that the Z term (league indicator) is insignificant (cannot reject Ho that the coefficient is equal to zero given other variables in the model) and thus does NOT contribute significantly to the model. In other words, because the effect of the leauge is insignificant after taking into account distance , then the probabilities of scoring field goals from a given distance are NOT statistically different for each league (i.e. probabilities are the same for each league). Note that also the 95% CI of odds ratio of scoring field goals in a given distance for the AFL vs NFL contains 1, suggesting that the odds ratio is not significantly different than one, meaning the odds of scoring given a distance for AFL vs NFL are not statistically different. 

# Problem 3


```r

# Import data
filename = "P014.txt"
mydata = read.table(filename, header = T)

# Look at data
names(mydata)
head(mydata)
nrow(mydata)
summary(mydata)

n = dim(mydata)[1]
```


## Part a

```r

# remove NETREV , since NETREV = PCREV - FEXP
fit = glm(RURAL ~ . - NETREV, family = binomial(link = "logit"), data = mydata)
summary(fit)
```

```
## 
## Call:
## glm(formula = RURAL ~ . - NETREV, family = binomial(link = "logit"), 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.035  -0.610   0.480   0.753   1.417  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)   
## (Intercept)  3.74e+00   1.38e+00    2.71   0.0067 **
## BED         -3.18e-02   2.83e-02   -1.12   0.2607   
## MCDAYS       1.59e-02   9.32e-03    1.70   0.0891 . 
## TDAYS       -6.69e-03   9.40e-03   -0.71   0.4764   
## PCREV        5.29e-05   1.26e-04    0.42   0.6751   
## NSAL        -7.15e-04   3.28e-04   -2.18   0.0295 * 
## FEXP         2.93e-04   2.63e-04    1.12   0.2647   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 67.083  on 51  degrees of freedom
## Residual deviance: 48.809  on 45  degrees of freedom
## AIC: 62.81
## 
## Number of Fisher Scoring iterations: 5
```

```r

# Asssess model fit ####
logLik(fit)
```

```
## 'log Lik.' -24.4 (df=7)
```

```r
deviance(fit)  # -2*logLik(fit)
```

```
## [1] 48.81
```

```r
fit$deviance
```

```
## [1] 48.81
```

```r
fit$null.deviance
```

```
## [1] 67.08
```

```r
G2 = fit$null.deviance - deviance(fit)
G2
```

```
## [1] 18.27
```

```r

pvalue = 1 - pchisq(fit$null.deviance - deviance(fit), 6)
pvalue
```

```
## [1] 0.005581
```

```r
q_crit = qchisq(p = 0.95, df = 6)
G2 > q_crit
```

```
## [1] TRUE
```

```r

# Null deviance = left unexplained after fittings beta's Bigger difference
# -> more explained How much addtw two predictors explain = 79

# 6 = 6 constraints H0: beta 1 = beta 2 ...beta 6
fit0 = glm(RURAL ~ 1, family = binomial(link = "logit"), data = mydata)
anova(fit0, fit, test = "Chisq")
```

```
## Analysis of Deviance Table
## 
## Model 1: RURAL ~ 1
## Model 2: RURAL ~ (BED + MCDAYS + TDAYS + PCREV + NSAL + FEXP + NETREV) - 
##     NETREV
##   Resid. Df Resid. Dev Df Deviance Pr(>Chi)   
## 1        51       67.1                        
## 2        45       48.8  6     18.3   0.0056 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

```r

```

The overall goodness of fit G^2 = 18.2748 > \( \chi \ 2 \)(0.05, 6) = 12.5916 (pvalue = 0.0056 < 0.05 ), so the null hypothesis that the coefficients of the predictors are zero is rejected at a 0.05 level. At least one of the predictors has an statistically significant effect on the response variable (probability rural vs non-rural), suggesting that rural vs non-rural facilities differ on at least one of the characteristics.


```r

# Find best model using AIC and BIC criteria
fitAIC = step(fit, direction='both')
```

```
## Start:  AIC=62.81
## RURAL ~ (BED + MCDAYS + TDAYS + PCREV + NSAL + FEXP + NETREV) - 
##     NETREV
## 
##          Df Deviance  AIC
## - PCREV   1     49.0 61.0
## - TDAYS   1     49.3 61.3
## - FEXP    1     50.2 62.2
## - BED     1     50.3 62.3
## <none>          48.8 62.8
## - MCDAYS  1     52.1 64.1
## - NSAL    1     54.8 66.8
## 
## Step:  AIC=60.99
## RURAL ~ BED + MCDAYS + TDAYS + NSAL + FEXP
## 
##          Df Deviance  AIC
## - TDAYS   1     49.4 59.4
## - BED     1     50.3 60.3
## - FEXP    1     50.5 60.5
## <none>          49.0 61.0
## - MCDAYS  1     52.5 62.5
## + PCREV   1     48.8 62.8
## - NSAL    1     54.8 64.8
## 
## Step:  AIC=59.36
## RURAL ~ BED + MCDAYS + NSAL + FEXP
## 
##          Df Deviance  AIC
## <none>          49.4 59.4
## - FEXP    1     51.4 59.4
## - BED     1     52.9 60.9
## + TDAYS   1     49.0 61.0
## + PCREV   1     49.3 61.3
## - MCDAYS  1     53.5 61.5
## - NSAL    1     57.0 65.0
```

```r
fitBIC = step(fit, direction='both', k=log(n))
```

```
## Start:  AIC=76.47
## RURAL ~ (BED + MCDAYS + TDAYS + PCREV + NSAL + FEXP + NETREV) - 
##     NETREV
## 
##          Df Deviance  AIC
## - PCREV   1     49.0 72.7
## - TDAYS   1     49.3 73.0
## - FEXP    1     50.2 73.9
## - BED     1     50.3 74.0
## - MCDAYS  1     52.1 75.8
## <none>          48.8 76.5
## - NSAL    1     54.8 78.5
## 
## Step:  AIC=72.69
## RURAL ~ BED + MCDAYS + TDAYS + NSAL + FEXP
## 
##          Df Deviance  AIC
## - TDAYS   1     49.4 69.1
## - BED     1     50.3 70.0
## - FEXP    1     50.5 70.2
## - MCDAYS  1     52.5 72.2
## <none>          49.0 72.7
## - NSAL    1     54.8 74.5
## + PCREV   1     48.8 76.5
## 
## Step:  AIC=69.11
## RURAL ~ BED + MCDAYS + NSAL + FEXP
## 
##          Df Deviance  AIC
## - FEXP    1     51.4 67.2
## - BED     1     52.9 68.7
## <none>          49.4 69.1
## - MCDAYS  1     53.5 69.3
## + TDAYS   1     49.0 72.7
## - NSAL    1     57.0 72.8
## + PCREV   1     49.3 73.0
## 
## Step:  AIC=67.24
## RURAL ~ BED + MCDAYS + NSAL
## 
##          Df Deviance  AIC
## - BED     1     53.4 65.3
## - MCDAYS  1     54.9 66.7
## <none>          51.4 67.2
## - NSAL    1     57.1 69.0
## + FEXP    1     49.4 69.1
## + TDAYS   1     50.5 70.2
## + PCREV   1     51.4 71.2
## 
## Step:  AIC=65.3
## RURAL ~ MCDAYS + NSAL
## 
##          Df Deviance  AIC
## - MCDAYS  1     55.4 63.3
## <none>          53.4 65.3
## + TDAYS   1     50.9 66.7
## + BED     1     51.4 67.2
## + FEXP    1     52.9 68.7
## + PCREV   1     53.0 68.8
## - NSAL    1     67.1 75.0
## 
## Step:  AIC=63.33
## RURAL ~ NSAL
## 
##          Df Deviance  AIC
## <none>          55.4 63.3
## + MCDAYS  1     53.4 65.3
## + FEXP    1     54.7 66.5
## + BED     1     54.9 66.7
## + PCREV   1     55.4 67.2
## + TDAYS   1     55.4 67.3
## - NSAL    1     67.1 71.0
```

```r
summary(fitAIC)
```

```
## 
## Call:
## glm(formula = RURAL ~ BED + MCDAYS + NSAL + FEXP, family = binomial(link = "logit"), 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -1.999  -0.589   0.453   0.734   1.439  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)   
## (Intercept)  3.644271   1.312794    2.78   0.0055 **
## BED         -0.036640   0.022469   -1.63   0.1030   
## MCDAYS       0.012620   0.007088    1.78   0.0750 . 
## NSAL        -0.000753   0.000317   -2.38   0.0174 * 
## FEXP         0.000344   0.000254    1.35   0.1755   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 67.083  on 51  degrees of freedom
## Residual deviance: 49.358  on 47  degrees of freedom
## AIC: 59.36
## 
## Number of Fisher Scoring iterations: 5
```

```r
summary(fitBIC)
```

```
## 
## Call:
## glm(formula = RURAL ~ NSAL, family = binomial(link = "logit"), 
##     data = mydata)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.066  -0.833   0.518   0.842   1.499  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  3.312614   0.969533    3.42  0.00063 ***
## NSAL        -0.000667   0.000220   -3.03  0.00246 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 67.083  on 51  degrees of freedom
## Residual deviance: 55.424  on 50  degrees of freedom
## AIC: 59.42
## 
## Number of Fisher Scoring iterations: 4
```

```r


mydata2 = within(mydata,{
  NETREV = NULL
})

mydata2 = cbind(mydata2[,-1],RURAL = mydata2[,1])
head(mydata2)
```

```
##   BED MCDAYS TDAYS PCREV NSAL FEXP RURAL
## 1 244    128   385 23521 5230 5334     0
## 2  59    155   203  9160 2459  493     1
## 3 120    281   392 21900 6304 6115     0
## 4 120    291   419 22354 6590 6346     0
## 5 120    238   363 17421 5362 6225     0
## 6  65    180   234 10531 3622  449     1
```

```r

# http://rstudio-pubs-static.s3.amazonaws.com/2897_9220b21cfc0c43a396ff9abf122bb351.html
# install.packages("bestglm")
# http://www2.uaem.mx/r-mirror/web/packages/bestglm/vignettes/bestglm.pdf
library(bestglm)
best_glm = bestglm(Xy = mydata2, IC = "AIC",family=binomial, method="exhaustive")
```

```
## Morgan-Tatar search since family is non-gaussian.
```

```r
names(best_glm)
```

```
## [1] "BestModel"   "BestModels"  "Bestq"       "qTable"      "Subsets"    
## [6] "Title"       "ModelReport"
```

```r
best_glm$BestModel
```

```
## 
## Call:  glm(formula = y ~ ., family = family, data = Xi, weights = weights)
## 
## Coefficients:
## (Intercept)       MCDAYS        TDAYS         NSAL  
##    3.035510     0.016095    -0.011539    -0.000527  
## 
## Degrees of Freedom: 51 Total (i.e. Null);  48 Residual
## Null Deviance:	    67.1 
## Residual Deviance: 50.9 	AIC: 58.9
```

```r
best_glm$BestModels
```

```
##     BED MCDAYS TDAYS PCREV NSAL  FEXP Criterion
## 1 FALSE   TRUE  TRUE FALSE TRUE FALSE     56.86
## 2  TRUE   TRUE FALSE FALSE TRUE  TRUE     57.36
## 3 FALSE  FALSE FALSE FALSE TRUE FALSE     57.42
## 4  TRUE   TRUE FALSE FALSE TRUE FALSE     57.44
## 5 FALSE   TRUE FALSE FALSE TRUE FALSE     57.45
```

```r
best_glm$Subsets
```

```
##    Intercept   BED MCDAYS TDAYS PCREV  NSAL  FEXP logLikelihood   AIC
## 0       TRUE FALSE  FALSE FALSE FALSE FALSE FALSE        -33.54 67.08
## 1       TRUE FALSE  FALSE FALSE FALSE  TRUE FALSE        -27.71 57.42
## 2       TRUE FALSE   TRUE FALSE FALSE  TRUE FALSE        -26.72 57.45
## 3*      TRUE FALSE   TRUE  TRUE FALSE  TRUE FALSE        -25.43 56.86
## 4       TRUE  TRUE   TRUE FALSE FALSE  TRUE  TRUE        -24.68 57.36
## 5       TRUE  TRUE   TRUE  TRUE FALSE  TRUE  TRUE        -24.49 58.99
## 6       TRUE  TRUE   TRUE  TRUE  TRUE  TRUE  TRUE        -24.40 60.81
```

```r

best_glm = bestglm(Xy = mydata2, IC = "BIC",family=binomial, method="exhaustive")
```

```
## Morgan-Tatar search since family is non-gaussian.
```

```r
names(best_glm)
```

```
## [1] "BestModel"   "BestModels"  "Bestq"       "qTable"      "Subsets"    
## [6] "Title"       "ModelReport"
```

```r
best_glm$BestModel
```

```
## 
## Call:  glm(formula = y ~ ., family = family, data = Xi, weights = weights)
## 
## Coefficients:
## (Intercept)         NSAL  
##    3.312614    -0.000667  
## 
## Degrees of Freedom: 51 Total (i.e. Null);  50 Residual
## Null Deviance:	    67.1 
## Residual Deviance: 55.4 	AIC: 59.4
```

```r
best_glm$BestModels
```

```
##     BED MCDAYS TDAYS PCREV NSAL  FEXP Criterion
## 1 FALSE  FALSE FALSE FALSE TRUE FALSE     59.38
## 2 FALSE   TRUE FALSE FALSE TRUE FALSE     61.35
## 3 FALSE  FALSE FALSE FALSE TRUE  TRUE     62.58
## 4 FALSE   TRUE  TRUE FALSE TRUE FALSE     62.72
## 5  TRUE  FALSE FALSE FALSE TRUE FALSE     62.76
```

```r
best_glm$Subsets
```

```
##    Intercept   BED MCDAYS TDAYS PCREV  NSAL  FEXP logLikelihood   BIC
## 0       TRUE FALSE  FALSE FALSE FALSE FALSE FALSE        -33.54 67.08
## 1*      TRUE FALSE  FALSE FALSE FALSE  TRUE FALSE        -27.71 59.38
## 2       TRUE FALSE   TRUE FALSE FALSE  TRUE FALSE        -26.72 61.35
## 3       TRUE FALSE   TRUE  TRUE FALSE  TRUE FALSE        -25.43 62.72
## 4       TRUE  TRUE   TRUE FALSE FALSE  TRUE  TRUE        -24.68 65.16
## 5       TRUE  TRUE   TRUE  TRUE FALSE  TRUE  TRUE        -24.49 68.74
## 6       TRUE  TRUE   TRUE  TRUE  TRUE  TRUE  TRUE        -24.40 72.52
```

```r

# best_glm = bestglm(Xy = mydata2, IC = "BICg",family=binomial, method="exhaustive")
# names(best_glm)
# best_glm$BestModel
# best_glm$Subsets
# 
# best_glm = bestglm(Xy = mydata2, IC = "BICq",family=binomial, method="exhaustive")
# names(best_glm)
# best_glm$BestModel
# best_glm$Subsets

best_glm = bestglm(Xy = mydata2, IC = "CV",family=binomial, method="exhaustive")
```

```
## Morgan-Tatar search since family is non-gaussian.
```

```r
names(best_glm)
```

```
## [1] "BestModel"   "BestModels"  "Bestq"       "qTable"      "Subsets"    
## [6] "Title"       "ModelReport"
```

```r
best_glm$BestModel
```

```
## 
## Call:  glm(formula = y ~ ., family = family, data = data.frame(Xy[, 
##     c(bestset[-1], FALSE), drop = FALSE], y = y))
## 
## Coefficients:
## (Intercept)         NSAL  
##    3.312614    -0.000667  
## 
## Degrees of Freedom: 51 Total (i.e. Null);  50 Residual
## Null Deviance:	    67.1 
## Residual Deviance: 55.4 	AIC: 59.4
```

```r
best_glm$BestModels
```

```
##   BestModels
## 1         NA
```

```r
best_glm$Subsets
```

```
##    Intercept   BED MCDAYS TDAYS PCREV  NSAL  FEXP logLikelihood     CV
## 0       TRUE FALSE  FALSE FALSE FALSE FALSE FALSE        -33.54 0.2445
## 1*      TRUE FALSE  FALSE FALSE FALSE  TRUE FALSE        -27.71 0.2079
## 2       TRUE FALSE   TRUE FALSE FALSE  TRUE FALSE        -26.72 0.2211
## 3       TRUE FALSE   TRUE  TRUE FALSE  TRUE FALSE        -25.43 0.2470
## 4       TRUE  TRUE   TRUE FALSE FALSE  TRUE  TRUE        -24.68 0.2773
## 5       TRUE  TRUE   TRUE  TRUE FALSE  TRUE  TRUE        -24.49 0.3185
## 6       TRUE  TRUE   TRUE  TRUE  TRUE  TRUE  TRUE        -24.40 0.3518
```

```r

# http://www.jstatsoft.org/v34/i12/paper
# http://r.789695.n4.nabble.com/glmulti-fails-because-of-rJava-td4100391.html
# http://www.dummies.com/how-to/content/how-to-install-and-configure-rstudio.html
# Need to use 32 bit
# install.packages("glmulti")
library(glmulti)
glmulti.logistic.out <-
    glmulti(RURAL~ ., data = mydata2,
            level = 1,               # No interaction considered
            method = "h",            # Exhaustive approach
            crit = "aic",            # AIC as criteria
            confsetsize = 5,         # Keep 5 best models
            plotty = F, report = F,  # No plot or interim reports
            fitfunction = "glm",     # glm function
            family = binomial)       # binomial family for logistic regression

## Show 5 best models (Use @ instead of $ for an S4 object)
glmulti.logistic.out@formulas
```

```
## [[1]]
## RURAL ~ 1 + MCDAYS + TDAYS + NSAL
## <environment: 0x04b4dfa0>
## 
## [[2]]
## RURAL ~ 1 + BED + MCDAYS + NSAL + FEXP
## <environment: 0x04b4dfa0>
## 
## [[3]]
## RURAL ~ 1 + NSAL
## <environment: 0x04b4dfa0>
## 
## [[4]]
## RURAL ~ 1 + BED + MCDAYS + NSAL
## <environment: 0x04b4dfa0>
## 
## [[5]]
## RURAL ~ 1 + MCDAYS + NSAL
## <environment: 0x04b4dfa0>
```

```r
summary(glmulti.logistic.out)
```

```
## $name
## [1] "glmulti.analysis"
## 
## $method
## [1] "h"
## 
## $fitting
## [1] "glm"
## 
## $crit
## [1] "aic"
## 
## $level
## [1] 1
## 
## $marginality
## [1] FALSE
## 
## $confsetsize
## [1] 5
## 
## $bestic
## [1] 58.86
## 
## $icvalues
## [1] 58.86 59.36 59.42 59.44 59.45
## 
## $bestmodel
## [1] "RURAL ~ 1 + MCDAYS + TDAYS + NSAL"
## 
## $modelweights
## [1] 0.2479 0.1935 0.1872 0.1861 0.1853
## 
## $includeobjects
## [1] TRUE
```

```r


plot(glmulti.logistic.out, type="p")
```

![plot of chunk unnamed-chunk-9](figure/unnamed-chunk-91.png) 

```r

# ranked relative evidence weight of the models
# They can be interpreted as probabilities for each model to be the best in the set
# A red vertical line is shown where the cumulated evidence weight reaches 95%
plot(glmulti.logistic.out, type = "w")
```

![plot of chunk unnamed-chunk-9](figure/unnamed-chunk-92.png) 

```r

# The third option plots for each term its estimated importance (or relative evidence weight), computed as the sum of the relative evidence weights of all models in which the term appears
plot(glmulti.logistic.out, type = "s")
```

![plot of chunk unnamed-chunk-9](figure/unnamed-chunk-93.png) 

```r

# Use LRT forward
add1(fit0,test="Chisq",scope=~BED+MCDAYS+TDAYS+PCREV+NSAL+FEXP,data=mydata2)
```

```
## Single term additions
## 
## Model:
## RURAL ~ 1
##        Df Deviance  AIC   LRT Pr(>Chi)    
## <none>        67.1 69.1                   
## BED     1     61.4 65.4  5.65  0.01746 *  
## MCDAYS  1     67.1 71.1  0.01  0.91749    
## TDAYS   1     63.7 67.7  3.40  0.06529 .  
## PCREV   1     63.1 67.1  3.94  0.04721 *  
## NSAL    1     55.4 59.4 11.66  0.00064 ***
## FEXP    1     66.3 70.3  0.79  0.37412    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

```r
fit1 = update(fit0,.~.+NSAL)
add1(fit1,test="Chisq",scope=~BED+MCDAYS+TDAYS+PCREV+NSAL+FEXP,data=mydata2)
```

```
## Single term additions
## 
## Model:
## RURAL ~ NSAL
##        Df Deviance  AIC   LRT Pr(>Chi)
## <none>        55.4 59.4               
## BED     1     54.9 60.9 0.563     0.45
## MCDAYS  1     53.4 59.4 1.979     0.16
## TDAYS   1     55.4 61.4 0.000     0.99
## PCREV   1     55.4 61.4 0.031     0.86
## FEXP    1     54.7 60.7 0.749     0.39
```

```r
# Don't add anything else, model is RURAL~NSAL

# Use LRT backward
drop1(fit,test="Chisq",data=mydata2)
```

```
## Single term deletions
## 
## Model:
## RURAL ~ (BED + MCDAYS + TDAYS + PCREV + NSAL + FEXP + NETREV) - 
##     NETREV
##        Df Deviance  AIC  LRT Pr(>Chi)  
## <none>        48.8 62.8                
## BED     1     50.3 62.3 1.47    0.226  
## MCDAYS  1     52.1 64.1 3.33    0.068 .
## TDAYS   1     49.3 61.3 0.51    0.476  
## PCREV   1     49.0 61.0 0.18    0.674  
## NSAL    1     54.8 66.8 5.97    0.015 *
## FEXP    1     50.2 62.2 1.35    0.246  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

```r
fit1 = update(fit,.~.-PCREV)
# get the same result as LRT forward

#ROC and Concordance
#http://web.expasy.org/pROC/screenshots.html
#http://thestatsgeek.com/2014/05/05/area-under-the-roc-curve-assessing-discrimination-in-logistic-regression/

# install.packages("pROC",dependencies= T)
# install.packages("Rcpp")

library(pROC)

# roc = area under curve = predicted power of model
# adding extra predictors better, but not adding interaction
fit1 =  glm(RURAL ~ NSAL, family=binomial(link="logit"), data=mydata) 
fit2  = glm(RURAL ~ MCDAYS+NSAL+TDAYS, family=binomial(link="logit"), data=mydata) 
rocobj1= plot.roc(mydata2$RURAL,
                  fit1$fitted.values, percent = TRUE,col="#1c61b6")
      
rocobj2= plot.roc(mydata2$RURAL,fit2$fitted.values, 
                  add=T, percent = TRUE,col="#008600")# T = don't erase previous grap

legend("bottomright", legend=c("NSAL", "NSAL+MCDAYS+TDAYS"), 
       col=c("#1c61b6", "#008600"), lwd=2, inset=0.05,cex=0.75)

testobj = roc.test(rocobj1, rocobj2)
text(50, 50, labels=paste("p-value =", format.pval(testobj$p.value)), adj=c(0, .5))
```

![plot of chunk unnamed-chunk-9](figure/unnamed-chunk-94.png) 


The best logistic regression model according to BIC is RURAL~NSAL, according to AIC is RURAL~MCDAYS+NSAL+TDAYS, and according to CV is RURAL~NSAL . Using LRT sequentially the best model is RURAL~NSAL. Note that by looking at the ROC curves and concordance index (Area Under the Curve = percent concordant adjusted for ties), we see that RURAL~MCDAYS+NSAL+TDAYS has higher discriminatory power than RURAL~NSAL. AUC can be interpreted as being the fraction of 0-1 pairs correctly classified by the model. However, the difference betwween the AUC's are not statistically different. Thus, the best mdodel seems to be RURAL~NSAL (including intercept).

## Part b

```r
# Stepwise regression to determine best model, start with all variables
library(MASS)
fit = lm(PCREV ~ ., data = mydata2)
summary(fit)
```

```
## 
## Call:
## lm(formula = PCREV ~ ., data = mydata2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -11886   -547    138   1179   7554 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -2839.585   1542.191   -1.84  0.07218 .  
## BED            43.723     18.610    2.35  0.02325 *  
## MCDAYS          3.216      8.677    0.37  0.71268    
## TDAYS          33.382      9.106    3.67  0.00065 ***
## NSAL            0.537      0.325    1.66  0.10469    
## FEXP            0.265      0.244    1.08  0.28372    
## RURAL         343.701    940.942    0.37  0.71662    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Residual standard error: 2700 on 45 degrees of freedom
## Multiple R-squared: 0.868,	Adjusted R-squared: 0.85 
## F-statistic: 49.3 on 6 and 45 DF,  p-value: <2e-16
```

```r
# Find best model using AIC and BIC criteria
fitAIC = step(fit, direction = "both")
```

```
## Start:  AIC=828.1
## PCREV ~ BED + MCDAYS + TDAYS + NSAL + FEXP + RURAL
## 
##          Df Sum of Sq      RSS AIC
## - RURAL   1    971845 3.29e+08 826
## - MCDAYS  1   1000333 3.29e+08 826
## - FEXP    1   8574053 3.36e+08 827
## <none>                3.28e+08 828
## - NSAL    1  19973594 3.48e+08 829
## - BED     1  40207259 3.68e+08 832
## - TDAYS   1  97893829 4.26e+08 840
## 
## Step:  AIC=826.3
## PCREV ~ BED + MCDAYS + TDAYS + NSAL + FEXP
## 
##          Df Sum of Sq      RSS AIC
## - MCDAYS  1   1617547 3.30e+08 825
## - FEXP    1   9376070 3.38e+08 826
## <none>                3.29e+08 826
## - NSAL    1  19242995 3.48e+08 827
## + RURAL   1    971845 3.28e+08 828
## - BED     1  39268507 3.68e+08 830
## - TDAYS   1  96922024 4.26e+08 838
## 
## Step:  AIC=824.5
## PCREV ~ BED + TDAYS + NSAL + FEXP
## 
##          Df Sum of Sq      RSS AIC
## - FEXP    1  1.25e+07 3.43e+08 824
## <none>                3.30e+08 825
## - NSAL    1  1.78e+07 3.48e+08 825
## + MCDAYS  1  1.62e+06 3.29e+08 826
## + RURAL   1  1.59e+06 3.29e+08 826
## - BED     1  3.83e+07 3.69e+08 828
## - TDAYS   1  2.99e+08 6.29e+08 856
## 
## Step:  AIC=824.5
## PCREV ~ BED + TDAYS + NSAL
## 
##          Df Sum of Sq      RSS AIC
## <none>                3.43e+08 824
## + FEXP    1  1.25e+07 3.30e+08 825
## + MCDAYS  1  4.74e+06 3.38e+08 826
## + RURAL   1  3.53e+06 3.39e+08 826
## - NSAL    1  3.23e+07 3.75e+08 827
## - BED     1  6.28e+07 4.06e+08 831
## - TDAYS   1  2.86e+08 6.29e+08 854
```

```r
fitBIC = step(fit, direction = "both", k = log(n))
```

```
## Start:  AIC=841.8
## PCREV ~ BED + MCDAYS + TDAYS + NSAL + FEXP + RURAL
## 
##          Df Sum of Sq      RSS AIC
## - RURAL   1    971845 3.29e+08 838
## - MCDAYS  1   1000333 3.29e+08 838
## - FEXP    1   8574053 3.36e+08 839
## - NSAL    1  19973594 3.48e+08 841
## <none>                3.28e+08 842
## - BED     1  40207259 3.68e+08 844
## - TDAYS   1  97893829 4.26e+08 851
## 
## Step:  AIC=838
## PCREV ~ BED + MCDAYS + TDAYS + NSAL + FEXP
## 
##          Df Sum of Sq      RSS AIC
## - MCDAYS  1   1617547 3.30e+08 834
## - FEXP    1   9376070 3.38e+08 836
## - NSAL    1  19242995 3.48e+08 837
## <none>                3.29e+08 838
## - BED     1  39268507 3.68e+08 840
## + RURAL   1    971845 3.28e+08 842
## - TDAYS   1  96922024 4.26e+08 847
## 
## Step:  AIC=834.3
## PCREV ~ BED + TDAYS + NSAL + FEXP
## 
##          Df Sum of Sq      RSS AIC
## - FEXP    1  1.25e+07 3.43e+08 832
## - NSAL    1  1.78e+07 3.48e+08 833
## <none>                3.30e+08 834
## - BED     1  3.83e+07 3.69e+08 836
## + MCDAYS  1  1.62e+06 3.29e+08 838
## + RURAL   1  1.59e+06 3.29e+08 838
## - TDAYS   1  2.99e+08 6.29e+08 864
## 
## Step:  AIC=832.3
## PCREV ~ BED + TDAYS + NSAL
## 
##          Df Sum of Sq      RSS AIC
## <none>                3.43e+08 832
## - NSAL    1  3.23e+07 3.75e+08 833
## + FEXP    1  1.25e+07 3.30e+08 834
## + MCDAYS  1  4.74e+06 3.38e+08 836
## + RURAL   1  3.53e+06 3.39e+08 836
## - BED     1  6.28e+07 4.06e+08 837
## - TDAYS   1  2.86e+08 6.29e+08 860
```

```r
summary(fitAIC)
```

```
## 
## Call:
## lm(formula = PCREV ~ BED + TDAYS + NSAL, data = mydata2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -11879   -706    -27   1174   7193 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -2187.591   1072.555   -2.04   0.0469 *  
## BED            48.024     16.196    2.97   0.0047 ** 
## TDAYS          34.807      5.498    6.33  7.8e-08 ***
## NSAL            0.568      0.267    2.12   0.0388 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Residual standard error: 2670 on 48 degrees of freedom
## Multiple R-squared: 0.862,	Adjusted R-squared: 0.853 
## F-statistic: 99.7 on 3 and 48 DF,  p-value: <2e-16
```

```r
summary(fitBIC)
```

```
## 
## Call:
## lm(formula = PCREV ~ BED + TDAYS + NSAL, data = mydata2)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -11879   -706    -27   1174   7193 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(>|t|)    
## (Intercept) -2187.591   1072.555   -2.04   0.0469 *  
## BED            48.024     16.196    2.97   0.0047 ** 
## TDAYS          34.807      5.498    6.33  7.8e-08 ***
## NSAL            0.568      0.267    2.12   0.0388 *  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Residual standard error: 2670 on 48 degrees of freedom
## Multiple R-squared: 0.862,	Adjusted R-squared: 0.853 
## F-statistic: 99.7 on 3 and 48 DF,  p-value: <2e-16
```

```r

x = mydata2[, -4]  # design matrix or use model.matrix(fullfit)
y = mydata2[, 4]  # response vector

# function returns best subset function with different criteria
modelSelection = function(x, y) {
    # Inputs: x = design matrix y = response vector
    n = length(y)  # number of observations
    p = dim(x)[2]  # number of predictors
    
    # Variable Selection Using Package
    library(leaps)
    
    # find the best subset
    reg_exh = regsubsets(x, y, nbest = 1, nvmax = n, method = "exhaustive")
    # summary(reg_exh,matrix.logical=TRUE) names(reg_exh)
    # names(summary(reg_exh))
    
    # get matrix with models
    models = summary(reg_exh)$which  # T/F -> multiply by 1 to get 1/0 (not needed)
    msize = as.numeric(apply(models, 1, sum))  # model size
    
    # compute criteria
    cp = summary(reg_exh)$cp
    cp = round(cp, 3)
    adjr2 = summary(reg_exh)$adjr2
    adjr2 = round(adjr2, 3)
    aic = n * log(summary(reg_exh)$rss/n) + 2 * msize
    aic = round(aic, 3)
    bic = n * log(summary(reg_exh)$rss/n) + msize * log(n)
    bic = round(bic, 3)
    # different from regsubsets, just differ by constant bic =
    # summary(reg_exh)$bic; bic = round(bic,3)
    
    # alternative optimizing various criteria leaps(x,y,nbest=1,method='Cp')
    # leaps(x,y,nbest=1,method='adjr2')
    
    # rank by criteria
    rk_cp = as.numeric(factor(cp))
    rk_adjr2 = vector(length = length(adjr2))
    rk_adjr2[order(adjr2, decreasing = TRUE)] = 1:length(adjr2)  # highest is better
    rk_aic = as.numeric(factor(aic))
    rk_bic = as.numeric(factor(bic))
    
    rk_tot = rk_cp + rk_adjr2 + rk_aic + rk_bic
    
    # create matrix and data frame of results
    results = cbind(msize, models, cp, adjr2, aic, bic, rk_cp, rk_adjr2, rk_aic, 
        rk_bic, rk_tot)
    
    colnames(results)[2] = "Int"
    
    results_df = data.frame(results)
    
    # display results
    results
    
    # alternative x1 = vector(length=length(cp)) x1[order(cp)] = 1:length(cp)
    
    # Models
    cp_model = c("intercept", colnames(x)[models[order(cp)[1], ][-1]])
    adjr2_model = c("intercept", colnames(x)[models[order(adjr2, decreasing = TRUE)[1], 
        ][-1]])
    aic_model = c("intercept", colnames(x)[models[order(aic)[1], ][-1]])
    bic_model = c("intercept", colnames(x)[models[order(bic)[1], ][-1]])
    
    cat("best cp model:\n", cp_model, "\n")
    cat("best adjr2 model:\n", adjr2_model, "\n")
    cat("best aic model:\n", aic_model, "\n")
    cat("best bic model:\n", bic_model, "\n")
    
    # Order results results[order(cp),]; # order by Cp
    # results[order(adjr2,decreasing=TRUE),]; # order by adjr2
    # results[order(aic),]; # order by BIC results[order(bic),]; # order by
    # BIC
    
    # alternative sort(cp, decreasing = FALSE,index.return=TRUE)$ix <->
    # order(cp)
    
    # plots
    
    plot(reg_exh, scale = "adjr2")
    plot(reg_exh, scale = "bic")
    plot(reg_exh, scale = "Cp")
    
    localenv = environment()
    
    require(ggplot2)
    require(grid)
    require(gridExtra)
    
    
    plot_vector = vector(mode = "list", length = 4)
    
    plot_vector[[1]] = ggplot(results_df, aes(x = results_df[[1]], y = results_df[[p + 
        3]]), environment = localenv) + geom_point(size = 4) + geom_line(aes(y = results_df[[p + 
        3]]), colour = "blue") + labs(x = colnames(results_df[1]), y = colnames(results_df[p + 
        3])) + scale_x_continuous(breaks = msize) + geom_point(data = results_df[order(cp)[1], 
        ], aes(x = msize, y = cp), colour = "red", size = 5)
    
    
    plot_vector[[2]] = ggplot(results_df, aes(x = results_df[[1]], y = results_df[[p + 
        3 + 1]]), environment = localenv) + geom_point(size = 4) + geom_line(aes(y = results_df[[p + 
        3 + 1]]), colour = "blue") + labs(x = colnames(results_df[1]), y = colnames(results_df[p + 
        3 + 1])) + scale_x_continuous(breaks = msize) + geom_point(data = results_df[order(adjr2, 
        decreasing = TRUE)[1], ], aes(x = msize, y = adjr2), colour = "red", 
        size = 5)
    
    plot_vector[[3]] = ggplot(results_df, aes(x = results_df[[1]], y = results_df[[p + 
        3 + 2]]), environment = localenv) + geom_point(size = 4) + geom_line(aes(y = results_df[[p + 
        3 + 2]]), colour = "blue") + labs(x = colnames(results_df[1]), y = colnames(results_df[p + 
        3 + 2])) + scale_x_continuous(breaks = msize) + geom_point(data = results_df[order(aic)[1], 
        ], aes(x = msize, y = aic), colour = "red", size = 5)
    
    plot_vector[[4]] = ggplot(results_df, aes(x = results_df[[1]], y = results_df[[p + 
        3 + 3]]), environment = localenv) + geom_point(size = 4) + geom_line(aes(y = results_df[[p + 
        3 + 3]]), colour = "blue") + labs(x = colnames(results_df[1]), y = colnames(results_df[p + 
        3 + 3])) + scale_x_continuous(breaks = msize) + geom_point(data = results_df[order(bic)[1], 
        ], aes(x = msize, y = bic), colour = "red", size = 5)
    
    
    grid.arrange(plot_vector[[1]], plot_vector[[2]], plot_vector[[3]], plot_vector[[4]], 
        ncol = 2, main = "Model Selection")
    
    
    
    return(results_df)
    
}

bestSubset = modelSelection(x, y)
```

```
## best cp model:
##  intercept BED TDAYS NSAL 
## best adjr2 model:
##  intercept BED TDAYS NSAL FEXP 
## best aic model:
##  intercept BED TDAYS NSAL 
## best bic model:
##  intercept BED TDAYS NSAL
```

![plot of chunk unnamed-chunk-10](figure/unnamed-chunk-101.png) ![plot of chunk unnamed-chunk-10](figure/unnamed-chunk-102.png) ![plot of chunk unnamed-chunk-10](figure/unnamed-chunk-103.png) ![plot of chunk unnamed-chunk-10](figure/unnamed-chunk-104.png) 

```r
bestSubset
```

```
##   msize Int BED MCDAYS TDAYS NSAL FEXP RURAL     cp adjr2   aic   bic
## 1     2   1   0      0     1    0    0     0 14.819 0.812 835.5 839.4
## 2     3   1   1      0     1    0    0     0  5.499 0.843 827.2 833.0
## 3     4   1   1      0     1    1    0     0  3.071 0.853 824.5 832.3
## 4     5   1   1      0     1    1    1     0  3.355 0.855 824.6 834.3
## 5     6   1   1      1     1    1    1     0  5.133 0.853 826.3 838.0
## 6     7   1   1      1     1    1    1     1  7.000 0.850 828.1 841.8
##   rk_cp rk_adjr2 rk_aic rk_bic rk_tot
## 1     6        6      6      5     23
## 2     4        5      4      2     15
## 3     1        2      1      1      5
## 4     2        1      2      3      8
## 5     3        3      3      4     13
## 6     5        4      5      6     20
```


All coefficients in the model are positive indicating higher values of the predictors values (hospital characteristics) have a positive effect in PCREV.For example, after controlling for the other hospital characteristics, a higher number of beds in home leads to an expected increase in total patient care revenue. Similar conclusions can be drawn from the coefficients of the other variables. However, only TDAYS seems to have a signficant effect on the patient care revenue. For the variable RURAL, it suggests that RURAL homes have a higher PCREV than non-rural. The effect seems to be very large, but not statistically significant. 

Both AIC and BIC stepwise methods give PCREV ~ BED + TDAYS + NSAL as the model (including intercept). This model is also best in CP, AIC, BIC and second best in adjusted R^2. Thus, this model seems to be the best model in predicting PREV. 


# Problem 4

```r

# Import data filename = 'p349-50.txt' mydata = read.table(filename,header
# = T) Note: this file is missing observations after 90, use other file
# instead

# Import data
filename = "diabetes.txt"
mydata = read.table(filename, header = T)


# Look at data
names(mydata)
head(mydata)
nrow(mydata)
summary(mydata)

```


## Part a

```r

# install.packages('mlogit')
library(mlogit)
```

```
## Loading required package: Formula
## Loading required package: maxLik
## Loading required package: miscTools
## 
## Please cite the 'maxLik' package as:
## Henningsen, Arne and Toomet, Ott (2011). maxLik: A package for maximum likelihood estimation in R. Computational Statistics 26(3), 443-458. DOI 10.1007/s00180-010-0217-1.
## 
## If you have questions, suggestions, or comments regarding the 'maxLik' package, please use a forum or 'tracker' at maxLik's R-Forge site:
## https://r-forge.r-project.org/projects/maxlik/
```

```r
diab = mlogit.data(data = mydata, choice = "CC", shape = "wide", varying = NULL)

# Table 12.9: Multinomial logistic Regression with IR,SSPG
fit = mlogit(CC ~ 0 | IR + SSPG, data = diab, reflevel = "3")
summary(fit)
```

```
## 
## Call:
## mlogit(formula = CC ~ 0 | IR + SSPG, data = diab, reflevel = "3", 
##     method = "nr", print.level = 0)
## 
## Frequencies of alternatives:
##     3     1     2 
## 0.524 0.228 0.248 
## 
## nr method
## 7 iterations, 0h:0m:0s 
## g'(-H)^-1g = 6.13E-05 
## successive function values within tolerance limits 
## 
## Coefficients :
##               Estimate Std. Error t-value Pr(>|t|)    
## 1:(intercept) -7.11066    1.68823   -4.21  2.5e-05 ***
## 2:(intercept) -4.54850    0.77147   -5.90  3.7e-09 ***
## 1:IR          -0.01343    0.00465   -2.89   0.0039 ** 
## 2:IR           0.00326    0.00229    1.42   0.1553    
## 1:SSPG         0.04259    0.00797    5.34  9.2e-08 ***
## 2:SSPG         0.01951    0.00445    4.38  1.2e-05 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Log-Likelihood: -72
## McFadden R^2:  0.514 
## Likelihood ratio test : chisq = 152 (p.value = <2e-16)
```

```r


# Calculate probabilities for each observation or summary(fit)$prob
Y.prob = fitted(fit, outcome = FALSE)
head(Y.prob)
```

```
##           3        1       2
## [1,] 0.9542 0.001534 0.04423
## [2,] 0.9323 0.004028 0.06363
## [3,] 0.8762 0.009182 0.11461
## [4,] 0.8532 0.004790 0.14201
## [5,] 0.7189 0.010336 0.27073
## [6,] 0.6438 0.072010 0.28418
```

```r

# classify to the category for which it has the highest estimated
# probabilities
n = dim(mydata)[1]
Y.hat = rep(0, n)
for (i in 1:n) {
    if (max(Y.prob[i, ]) == Y.prob[i, 1]) {
        Y.hat[i] = 3
    } else if (max(Y.prob[i, ]) == Y.prob[i, 2]) {
        Y.hat[i] = 1
    } else if (max(Y.prob[i, ]) == Y.prob[i, 3]) {
        Y.hat[i] = 2
    }
}
Y.hat
```

```
##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3
##  [36] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3 3 2 3 3 1 3 3 1 2 3
##  [71] 2 3 3 3 2 3 3 3 3 3 3 2 3 1 3 2 2 2 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 3
## [106] 2 3 2 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 3 3 2 2 1 1 1
## [141] 1 3 1 1 1
```

```r

# Table 12.10: Classification table
ctable = table(mydata$CC, Y.hat)
ctable = addmargins(ctable)
ctable
```

```
##      Y.hat
##         1   2   3 Sum
##   1    27   3   3  33
##   2     1  22  13  36
##   3     2   5  69  76
##   Sum  30  30  85 145
```

```r

correct.rate = sum(diag(ctable)[1:3])/n
correct.rate
```

```
## [1] 0.8138
```

```r

## include RW install.packages('mlogit')
library(mlogit)
diab = mlogit.data(data = mydata, choice = "CC", shape = "wide", varying = NULL)

# Table 12.9: Multinomial logistic Regression with IR,SSPG
fit = mlogit(CC ~ 0 | IR + SSPG + RW, data = diab, reflevel = "3")
summary(fit)
```

```
## 
## Call:
## mlogit(formula = CC ~ 0 | IR + SSPG + RW, data = diab, reflevel = "3", 
##     method = "nr", print.level = 0)
## 
## Frequencies of alternatives:
##     3     1     2 
## 0.524 0.228 0.248 
## 
## nr method
## 7 iterations, 0h:0m:0s 
## g'(-H)^-1g = 0.00028 
## successive function values within tolerance limits 
## 
## Coefficients :
##               Estimate Std. Error t-value Pr(>|t|)    
## 1:(intercept) -1.84461    3.46346   -0.53  0.59432    
## 2:(intercept) -7.61542    2.33563   -3.26  0.00111 ** 
## 1:IR          -0.01335    0.00502   -2.66  0.00780 ** 
## 2:IR           0.00359    0.00235    1.53  0.12680    
## 1:SSPG         0.04550    0.00924    4.92  8.5e-07 ***
## 2:SSPG         0.01641    0.00498    3.29  0.00098 ***
## 1:RW          -5.86746    3.86658   -1.52  0.12915    
## 2:RW           3.47277    2.44616    1.42  0.15570    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Log-Likelihood: -68.4
## McFadden R^2:  0.538 
## Likelihood ratio test : chisq = 159 (p.value = <2e-16)
```

```r

# Calculate probabilities for each observation or summary(fit)$prob
Y.prob = fitted(fit, outcome = FALSE)
head(Y.prob)
```

```
##           3        1       2
## [1,] 0.9664 0.003074 0.03053
## [2,] 0.9305 0.003717 0.06580
## [3,] 0.8835 0.009896 0.10662
## [4,] 0.8179 0.002765 0.17937
## [5,] 0.7118 0.008652 0.27955
## [6,] 0.6283 0.257438 0.11425
```

```r

# classify to the category for which it has the highest estimated
# probabilities
n = dim(mydata)[1]
Y.hat = rep(0, n)
for (i in 1:n) {
    if (max(Y.prob[i, ]) == Y.prob[i, 1]) {
        Y.hat[i] = 3
    } else if (max(Y.prob[i, ]) == Y.prob[i, 2]) {
        Y.hat[i] = 1
    } else if (max(Y.prob[i, ]) == Y.prob[i, 3]) {
        Y.hat[i] = 2
    }
}
Y.hat
```

```
##   [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 3 3 3 3 3 3
##  [36] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 3 3 3 3 3 3 3 3 3 3 2 2 3 2 3 3 1 2 3
##  [71] 2 3 3 3 2 3 3 3 3 3 3 2 2 2 3 2 2 3 2 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 3
## [106] 2 3 3 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 3 3 2 2 1 1 1
## [141] 1 3 1 1 1
```

```r

# Table 12.10: Classification table
ctable2 = table(mydata$CC, Y.hat)
ctable2 = addmargins(ctable2)
ctable2
```

```
##      Y.hat
##         1   2   3 Sum
##   1    27   3   3  33
##   2     0  24  12  36
##   3     2   5  69  76
##   Sum  29  32  84 145
```

```r

correct.rate2 = sum(diag(ctable2)[1:3])/n
correct.rate2
```

```
## [1] 0.8276
```


The classification rate for multinomial logistic model CC~IR+SSPG is 81.3793 %, while the rate for CC~IR+SSPG+RW is 82.7586 %, which is an improvement of just  1.3793%. Thus, the inclusion of RW does not result in a substantial improvement in the classification rate from the multinomial logistic model using IR and SSPG. 

## Part b

```r

# Table 12.11: Ordinal Logistic Regression with IR, SSPG
# install.packages('ordinal')
library(ordinal)
mydata$CC.ordered = as.ordered(mydata$CC)
fit = clm(CC.ordered ~ IR + SSPG, data = mydata)
fit1 = fit
summary(fit)
```

```
## formula: CC.ordered ~ IR + SSPG
## data:    mydata
## 
##  link  threshold nobs logLik AIC    niter max.grad cond.H 
##  logit flexible  145  -81.75 171.50 6(0)  1.48e-12 2.6e+06
## 
## Coefficients:
##      Estimate Std. Error z value Pr(>|z|)    
## IR    0.00406    0.00174    2.33     0.02 *  
## SSPG -0.02814    0.00359   -7.83  4.7e-15 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Threshold coefficients:
##     Estimate Std. Error z value
## 1|2   -6.794      0.857   -7.93
## 2|3   -4.189      0.662   -6.33
```

```r

# Table 12.12: Classification table
Y.hat = predict(fit, data = mydata, type = "class")$fit
ctable = table(mydata$CC, Y.hat)
ctable = addmargins(ctable)
ctable
```

```
##      Y.hat
##         1   2   3 Sum
##   1    26   5   2  33
##   2     3  20  13  36
##   3     0   8  68  76
##   Sum  29  33  83 145
```

```r

correct.rate = sum(diag(ctable)[1:3])/n
correct.rate
```

```
## [1] 0.7862
```

```r

# add RW

# Table 12.11: Ordinal Logistic Regression with IR, SSPG
# install.packages('ordinal')
library(ordinal)
mydata$CC.ordered = as.ordered(mydata$CC)
fit = clm(CC.ordered ~ IR + SSPG + RW, data = mydata)
summary(fit)
```

```
## formula: CC.ordered ~ IR + SSPG + RW
## data:    mydata
## 
##  link  threshold nobs logLik AIC    niter max.grad cond.H 
##  logit flexible  145  -81.21 172.42 6(0)  8.56e-13 2.1e+07
## 
## Coefficients:
##      Estimate Std. Error z value Pr(>|z|)    
## IR    0.00377    0.00178    2.11    0.035 *  
## SSPG -0.02927    0.00383   -7.65    2e-14 ***
## RW    1.92002    1.86155    1.03    0.302    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Threshold coefficients:
##     Estimate Std. Error z value
## 1|2    -5.16       1.77   -2.91
## 2|3    -2.53       1.72   -1.47
```

```r

# Table 12.12: Classification table
Y.hat = predict(fit, data = mydata, type = "class")$fit
ctable2 = table(mydata$CC, Y.hat)
ctable2 = addmargins(ctable2)
ctable2
```

```
##      Y.hat
##         1   2   3 Sum
##   1    26   5   2  33
##   2     2  21  13  36
##   3     0   9  67  76
##   Sum  28  35  82 145
```

```r

correct.rate2 = sum(diag(ctable2)[1:3])/n
correct.rate2
```

```
## [1] 0.7862
```

```r

anova(fit1, fit)
```

```
## Likelihood ratio tests of cumulative link models:
##  
##      formula:                    link: threshold:
## fit1 CC.ordered ~ IR + SSPG      logit flexible  
## fit  CC.ordered ~ IR + SSPG + RW logit flexible  
## 
##      no.par AIC logLik LR.stat df Pr(>Chisq)
## fit1      4 171  -81.7                      
## fit       5 172  -81.2    1.08  1        0.3
```


The classification rate for ordinal logistic model CC~IR+SSPG is 78.6207 %, while the rate for CC~IR+SSPG+RW is 78.6207 %, which is an improvement of just  0%. Thus, the inclusion of RW does not result in a substantial improvement in the classification rate from the ordinal logistic model using IR and SSPG. 

The p-value > 0.05 (do not reject null that coefficient of RW is zero) indicates that the model without RW is better than the one with RW. Thus, there in no substantial improvment in the fit by adding RW. 

# Problem 5


```r
# Import data
filename = "MAMMOGRAPHY+DATA.csv"
mydata = read.csv(filename, header = T)

# Look at data
names(mydata)
head(mydata)
nrow(mydata)
summary(mydata)
```


## Part a

```r
is_even = function(x) x%%2 == 0

train_data = mydata[!sapply(mydata$OBS, is_even), c("OBS", "ME", "HIST", "PB")]
test_data = mydata[sapply(mydata$OBS, is_even), c("OBS", "ME", "HIST", "PB")]
head(train_data)
```

```
##    OBS ME HIST PB
## 1    1  0    0  7
## 3    3  0    1  8
## 5    5  2    0  7
## 7    7  2    0  6
## 9    9  0    0  6
## 11  11  0    0  8
```

```r
head(test_data)
```

```
##    OBS ME HIST PB
## 2    2  0    0 11
## 4    4  1    0 11
## 6    6  0    0  7
## 8    8  0    0  6
## 10  10  1    0  6
## 12  12  0    1  6
```

```r


# install.packages('mlogit')
library(mlogit)
mammo = mlogit.data(data = train_data, choice = "ME", shape = "wide", varying = NULL)
head(mammo, 20)
```

```
##      OBS    ME HIST PB chid alt
## 1.0    1  TRUE    0  7    1   0
## 1.1    1 FALSE    0  7    1   1
## 1.2    1 FALSE    0  7    1   2
## 3.0    3  TRUE    1  8    3   0
## 3.1    3 FALSE    1  8    3   1
## 3.2    3 FALSE    1  8    3   2
## 5.0    5 FALSE    0  7    5   0
## 5.1    5 FALSE    0  7    5   1
## 5.2    5  TRUE    0  7    5   2
## 7.0    7 FALSE    0  6    7   0
## 7.1    7 FALSE    0  6    7   1
## 7.2    7  TRUE    0  6    7   2
## 9.0    9  TRUE    0  6    9   0
## 9.1    9 FALSE    0  6    9   1
## 9.2    9 FALSE    0  6    9   2
## 11.0  11  TRUE    0  8   11   0
## 11.1  11 FALSE    0  8   11   1
## 11.2  11 FALSE    0  8   11   2
## 13.0  13  TRUE    0  6   13   0
## 13.1  13 FALSE    0  6   13   1
```

```r
head(mydata, 15)
```

```
##    OBS ME SYMPT PB HIST BSE DETC
## 1    1  0     3  7    0   1    2
## 2    2  0     2 11    0   1    3
## 3    3  0     3  8    1   1    3
## 4    4  1     3 11    0   1    3
## 5    5  2     4  7    0   1    3
## 6    6  0     3  7    0   1    3
## 7    7  2     4  6    0   1    2
## 8    8  0     4  6    0   1    3
## 9    9  0     2  6    0   1    3
## 10  10  1     4  6    0   1    3
## 11  11  0     4  8    0   1    2
## 12  12  0     3  6    1   0    3
## 13  13  0     4  6    0   1    3
## 14  14  0     1  5    1   1    3
## 15  15  0     2  8    0   0    2
```

```r
# Table 12.9: Multinomial logistic Regression with IR,SSPG
fit = mlogit(ME ~ 0 | PB + HIST, data = mammo, reflevel = "0")
summary(fit)
```

```
## 
## Call:
## mlogit(formula = ME ~ 0 | PB + HIST, data = mammo, reflevel = "0", 
##     method = "nr", print.level = 0)
## 
## Frequencies of alternatives:
##     0     1     2 
## 0.602 0.238 0.160 
## 
## nr method
## 4 iterations, 0h:0m:0s 
## g'(-H)^-1g = 0.000273 
## successive function values within tolerance limits 
## 
## Coefficients :
##               Estimate Std. Error t-value Pr(>|t|)   
## 1:(intercept)   1.0357     0.7153    1.45   0.1476   
## 2:(intercept)   0.2425     0.7919    0.31   0.7594   
## 1:PB           -0.2997     0.0983   -3.05   0.0023 **
## 2:PB           -0.2250     0.1064   -2.11   0.0345 * 
## 1:HIST          1.6554     0.5268    3.14   0.0017 **
## 2:HIST          1.0648     0.6300    1.69   0.0910 . 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Log-Likelihood: -182
## McFadden R^2:  0.0626 
## Likelihood ratio test : chisq = 24.2 (p.value = 7.15e-05)
```

```r

# Calculate probabilities for each observation Y.prob = fitted(fit,
# outcome= FALSE)

# Need a function because want to find prob in a different data set than
# training function predict probabilities test data has to be in same
# order fit variables
predict_mlogit = function(testdata, fit) {
    beta = fit$coeff
    beta1 = beta[seq(1, length(beta), 2)]
    beta2 = beta[seq(2, length(beta), 2)]
    exp1 = exp(as.matrix(cbind(rep(1, dim(testdata)[1]), testdata)) %*% beta1)
    exp2 = exp(as.matrix(cbind(rep(1, dim(testdata)[1]), testdata)) %*% beta2)
    pi1 = exp1/(1 + exp1 + exp2)
    pi2 = exp2/(1 + exp1 + exp2)
    pi0 = 1/(1 + exp1 + exp2)
    prob = cbind(pi0 = pi0, pi1 = pi1, pi2 = pi2)
    colnames(prob) = c(0, 1, 2)
    return(prob)
}

Y.prob = predict_mlogit(test_data[, c(4, 3)], fit)
head(Y.prob)
```

```
##         0       1      2
## 2  0.8255 0.08605 0.0885
## 4  0.8255 0.08605 0.0885
## 6  0.6213 0.21480 0.1639
## 8  0.5565 0.25963 0.1838
## 10 0.5565 0.25963 0.1838
## 12 0.2273 0.55502 0.2177
```

```r

# classify to the category for which it has the highest estimated
# probabilities

n_train = dim(train_data)[1]
n_test = dim(test_data)[1]

Y.hat = rep(0, n_test)
for (i in 1:n_test) {
    if (max(Y.prob[i, ]) == Y.prob[i, 1]) {
        Y.hat[i] = 0
    } else if (max(Y.prob[i, ]) == Y.prob[i, 2]) {
        Y.hat[i] = 1
    } else if (max(Y.prob[i, ]) == Y.prob[i, 3]) {
        Y.hat[i] = 2
    }
}
Y.hat
```

```
##   [1] 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
##  [36] 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
##  [71] 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0
## [106] 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0
## [141] 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
## [176] 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0
```

```r



# Table 12.10: Classification table
ctable = table(test_data$ME, Y.hat)
ctable = cbind(ctable, `2` = c(0, 0, 0))
ctable = addmargins(ctable)
ctable
```

```
##       0  1 2 Sum
## 0   106  4 0 110
## 1    48  7 0  55
## 2    37  4 0  41
## Sum 191 15 0 206
```

```r

correct.rate = sum(diag(ctable)[1:3])/n_test
correct.rate
```

```
## [1] 0.5485
```

```r
1 - correct.rate
```

```
## [1] 0.4515
```

```r

miss0 = sum(ctable[1, -4][-1])/ctable[1, 4]
miss1 = sum(ctable[2, -4][-2])/ctable[2, 4]
miss2 = sum(ctable[3, -4][-3])/ctable[3, 4]
miss0
```

```
## [1] 0.03636
```

```r
miss1
```

```
## [1] 0.8727
```

```r
miss2
```

```
## [1] 1
```


The misclassification rate is 45.1456 %. The observed (rows) and predicted (columns) outcomes is shown above in the tabulation. The break down of the misclassification for the three categories is:
- Never (0): 3.6364%
- within the past year (1): 87.2727%
- More than one year ago (2):  100%

## Part b

```r

# install.packages('ordinal')
library(ordinal)
train_data$ME.ordered = factor(train_data$ME, levels = c(0, 2, 1), ordered = T)
test_data$ME.ordered = factor(test_data$ME, levels = c(0, 2, 1), ordered = T)
fit = clm(ME.ordered ~ PB + HIST, data = train_data)
summary(fit)
```

```
## formula: ME.ordered ~ PB + HIST
## data:    train_data
## 
##  link  threshold nobs logLik  AIC    niter max.grad cond.H 
##  logit flexible  206  -181.61 371.23 6(0)  5.42e-13 1.7e+03
## 
## Coefficients:
##      Estimate Std. Error z value Pr(>|z|)    
## PB    -0.2626     0.0773   -3.40  0.00068 ***
## HIST   1.3724     0.4202    3.27  0.00109 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 
## 
## Threshold coefficients:
##     Estimate Std. Error z value
## 0|2   -1.344      0.575   -2.34
## 2|1   -0.510      0.570   -0.90
```

```r

# Table 12.12: Classification table
Y.hat = predict(fit, newdata = test_data, type = "class")$fit
ctable = table(test_data$ME.ordered, Y.hat)
ctable = addmargins(ctable)
ctable
```

```
##      Y.hat
##         0   2   1 Sum
##   0   106   0   4 110
##   2    37   0   4  41
##   1    48   0   7  55
##   Sum 191   0  15 206
```

```r

correct.rate = sum(diag(ctable)[1:3])/n_test
correct.rate
```

```
## [1] 0.5485
```

```r
1 - correct.rate
```

```
## [1] 0.4515
```

```r

miss0 = sum(ctable[1, -4][-1])/ctable[1, 4]
miss1 = sum(ctable[3, -4][-3])/ctable[3, 4]
miss2 = sum(ctable[2, -4][-2])/ctable[2, 4]
miss0
```

```
## [1] 0.03636
```

```r
miss1
```

```
## [1] 0.8727
```

```r
miss2
```

```
## [1] 1
```

The misclassification rate is 45.1456 %. The observed (rows) and predicted (columns) outcomes is shown above in the tabulation. The break down of the misclassification for the three categories is:
- Never (0): 3.6364%
- within the past year (1): 87.2727%
- More than one year ago (2):  100%

Compared to the multinomial logistic model, the overall misclassification rate is the same. Looking at the breakdown, the misclassification of the ordinal model is the same as the multinomial. We do not get better predictions. 
